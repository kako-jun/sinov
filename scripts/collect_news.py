#!/usr/bin/env python
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®šæœŸçš„ã«å®Ÿè¡Œã—ã¦npcs/data/bulletin_board/news.jsonã«æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¿å­˜
è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ã—ã¦LLMã§è¦ç´„ã‚’ç”Ÿæˆ
"""

import asyncio
import sys
import uuid
from datetime import datetime, timedelta
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import Settings
from src.domain.news import NewsItem, ReporterConfig
from src.infrastructure.external import ArticleFetcher, ArticleSummarizer, RSSClient, TrendScraper
from src.infrastructure.llm import OllamaProvider
from src.infrastructure.storage.bulletin_repo import BulletinRepository

# è¨˜è€…è¨­å®š
REPORTERS = {
    "reporter_tech": ReporterConfig(
        id="reporter_tech",
        specialty="ITãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
        sources=[
            {
                "name": "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ï¼‰",
                "url": "https://b.hatena.ne.jp/hotentry/it.rss",
            },
        ],
        include_keywords=[
            "ã‚¢ãƒ—ãƒª",
            "ãƒ„ãƒ¼ãƒ«",
            "ã‚µãƒ¼ãƒ“ã‚¹",
            "é–‹ç™º",
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°",
            "AI",
            "æ©Ÿæ¢°å­¦ç¿’",
            "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹",
            "ãƒªãƒªãƒ¼ã‚¹",
        ],
        exclude_keywords=["æ”¿æ²»", "é¸æŒ™", "é€®æ•", "äº‹ä»¶", "è¨´è¨Ÿ", "ç‚ä¸Š"],
        anonymize=True,
    ),
    "reporter_game": ReporterConfig(
        id="reporter_game",
        specialty="ã‚²ãƒ¼ãƒ ",
        sources=[
            {
                "name": "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆã‚²ãƒ¼ãƒ ï¼‰",
                "url": "https://b.hatena.ne.jp/hotentry/game.rss",
            },
        ],
        include_keywords=[
            "ã‚²ãƒ¼ãƒ ",
            "ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¼",
            "Steam",
            "Nintendo",
            "PlayStation",
            "ãƒªãƒªãƒ¼ã‚¹",
            "ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
        ],
        exclude_keywords=["ç‚ä¸Š", "è¨´è¨Ÿ", "ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«"],
        anonymize=True,
    ),
    "reporter_creative": ReporterConfig(
        id="reporter_creative",
        specialty="å‰µä½œãƒ»ã‚¢ãƒ¼ãƒˆ",
        sources=[
            {
                "name": "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆã‚¨ãƒ³ã‚¿ãƒ¡ï¼‰",
                "url": "https://b.hatena.ne.jp/hotentry/entertainment.rss",
            },
        ],
        include_keywords=[
            "ã‚¤ãƒ©ã‚¹ãƒˆ",
            "çµµ",
            "æ¼«ç”»",
            "ã‚¢ãƒ‹ãƒ¡",
            "ãƒ‡ã‚¶ã‚¤ãƒ³",
            "éŸ³æ¥½",
            "DTM",
            "ä½œæ›²",
            "å°èª¬",
            "å‰µä½œ",
        ],
        exclude_keywords=["ç‚ä¸Š", "æ‰¹åˆ¤", "è¨´è¨Ÿ", "ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«", "æ”¿æ²»", "äº‹ä»¶"],
        anonymize=True,
    ),
    "reporter_general": ReporterConfig(
        id="reporter_general",
        specialty="ä¸€èˆ¬æ™‚äº‹ï¼ˆITãƒ»å‰µä½œç³»ï¼‰",
        sources=[
            {
                "name": "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆç·åˆï¼‰",
                "url": "https://b.hatena.ne.jp/hotentry/all.rss",
            },
        ],
        include_keywords=[
            "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
            "ã‚¢ãƒ—ãƒª",
            "ã‚µãƒ¼ãƒ“ã‚¹",
            "å‰µä½œ",
            "ãƒ„ãƒ¼ãƒ«",
            "é–‹ç™º",
        ],
        exclude_keywords=[
            "æ”¿æ²»",
            "é¸æŒ™",
            "å®—æ•™",
            "äº‹ä»¶",
            "é€®æ•",
            "ç‚ä¸Š",
            "è¨´è¨Ÿ",
            "æˆ¦äº‰",
        ],
        anonymize=True,
    ),
}


async def collect_news_for_reporter(
    reporter_id: str,
    config: ReporterConfig,
    rss_client: RSSClient,
    fetcher: ArticleFetcher,
    summarizer: ArticleSummarizer,
) -> list[NewsItem]:
    """è¨˜è€…ã”ã¨ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ï¼ˆæœ¬æ–‡å–å¾—ãƒ»è¦ç´„ä»˜ãï¼‰"""
    all_items = []

    for source in config.sources:
        print(f"  Fetching from {source['name']}...")
        rss_items = rss_client.fetch(source["url"], limit=10)

        for item in rss_items:
            title = item.title
            rss_summary = item.summary

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if not config.should_include(title + " " + rss_summary):
                continue

            # è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
            url = item.link
            summary = rss_summary[:200] if rss_summary else ""  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

            if url:
                content = fetcher.fetch_content(url)
                if content and len(content) > 100:
                    # LLMã§è¦ç´„ç”Ÿæˆ
                    summary = await summarizer.summarize(title, content)
                    print(f"    ğŸ“ è¦ç´„ç”Ÿæˆ: {title[:30]}...")

            news_item = NewsItem(
                id=f"news_{uuid.uuid4().hex[:8]}",
                title=title,
                summary=summary,
                category=config.specialty.lower().replace("ãƒ»", "_"),
                source=reporter_id,
                original_url=url or None,
                posted_at=datetime.now(),
                expires_at=datetime.now() + timedelta(days=2),
            )
            all_items.append(news_item)

    return all_items


def collect_trends(scraper: TrendScraper) -> list[NewsItem]:
    """Google Trendsã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åé›†"""
    all_items = []

    print("  Fetching Google Trends (Japan)...")
    trends = scraper.fetch_trends(limit=10)

    for trend in trends:
        news_item = NewsItem(
            id=f"trend_{uuid.uuid4().hex[:8]}",
            title=trend.name,
            summary=f"Googleãƒˆãƒ¬ãƒ³ãƒ‰å…¥ã‚Šï¼ˆ{trend.category or 'è©±é¡Œ'}ï¼‰",
            category="trend",
            source="reporter_trend",
            original_url=None,
            posted_at=datetime.now(),
            expires_at=datetime.now() + timedelta(hours=12),
        )
        all_items.append(news_item)

    return all_items


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ“° Collecting news from all reporters...")

    # åˆæœŸåŒ–
    settings = Settings()
    bulletin_repo = BulletinRepository(Path("npcs/data/bulletin_board"))
    rss_client = RSSClient()
    trend_scraper = TrendScraper()
    fetcher = ArticleFetcher()

    # LLMåˆæœŸåŒ–
    print(f"  Initializing LLM ({settings.ollama_model})...")
    llm = OllamaProvider(settings.ollama_host, settings.ollama_model)
    summarizer = ArticleSummarizer(llm)

    # æœŸé™åˆ‡ã‚Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‰Šé™¤
    removed = bulletin_repo.cleanup_expired()
    if removed > 0:
        print(f"  Removed {removed} expired news items")

    total_added = 0

    # RSSè¨˜è€…ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
    for reporter_id, config in REPORTERS.items():
        print(f"\nğŸ“ {reporter_id} ({config.specialty}):")

        news_items = await collect_news_for_reporter(
            reporter_id, config, rss_client, fetcher, summarizer
        )

        for item in news_items[:5]:  # å„è¨˜è€…æœ€å¤§5ä»¶
            bulletin_repo.add_news_item(item)
            print(f"    + {item.title[:50]}...")
            total_added += 1

    # Googleãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åé›†
    print("\nğŸ“ reporter_trend (Googleãƒˆãƒ¬ãƒ³ãƒ‰):")
    trend_items = collect_trends(trend_scraper)

    for item in trend_items[:5]:  # æœ€å¤§5ä»¶
        bulletin_repo.add_news_item(item)
        print(f"    + {item.title[:50]}...")
        total_added += 1

    print(f"\nâœ… Added {total_added} news items")

    # æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¡¨ç¤º
    recent = bulletin_repo.get_recent_news(5)
    print(f"\nğŸ“‹ Recent news ({len(recent)} items):")
    for item in recent:
        print(f"  - [{item.category}] {item.title[:40]}...")


if __name__ == "__main__":
    asyncio.run(main())

#!/usr/bin/env python
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®šæœŸçš„ã«å®Ÿè¡Œã—ã¦bots/data/bulletin_board/news.jsonã«æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¿å­˜
"""

import sys
import uuid
from datetime import datetime, timedelta
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.domain.news import NewsItem, ReporterConfig
from src.infrastructure.storage.bulletin_repo import BulletinRepository

# è¨˜è€…è¨­å®š
REPORTERS = {
    "reporter_tech": ReporterConfig(
        id="reporter_tech",
        specialty="ITãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
        sources=[
            {"name": "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ï¼‰", "url": "https://b.hatena.ne.jp/hotentry/it.rss"},
        ],
        include_keywords=["ã‚¢ãƒ—ãƒª", "ãƒ„ãƒ¼ãƒ«", "ã‚µãƒ¼ãƒ“ã‚¹", "é–‹ç™º", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "AI", "æ©Ÿæ¢°å­¦ç¿’", "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹", "ãƒªãƒªãƒ¼ã‚¹"],
        exclude_keywords=["æ”¿æ²»", "é¸æŒ™", "é€®æ•", "äº‹ä»¶", "è¨´è¨Ÿ", "ç‚ä¸Š"],
        anonymize=True,
    ),
    "reporter_game": ReporterConfig(
        id="reporter_game",
        specialty="ã‚²ãƒ¼ãƒ ",
        sources=[
            {"name": "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆã‚²ãƒ¼ãƒ ï¼‰", "url": "https://b.hatena.ne.jp/hotentry/game.rss"},
        ],
        include_keywords=["ã‚²ãƒ¼ãƒ ", "ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¼", "Steam", "Nintendo", "PlayStation", "ãƒªãƒªãƒ¼ã‚¹", "ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ"],
        exclude_keywords=["ç‚ä¸Š", "è¨´è¨Ÿ", "ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«"],
        anonymize=True,
    ),
    "reporter_creative": ReporterConfig(
        id="reporter_creative",
        specialty="å‰µä½œãƒ»ã‚¢ãƒ¼ãƒˆ",
        sources=[
            {"name": "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆã‚¨ãƒ³ã‚¿ãƒ¡ï¼‰", "url": "https://b.hatena.ne.jp/hotentry/entertainment.rss"},
        ],
        include_keywords=["ã‚¤ãƒ©ã‚¹ãƒˆ", "çµµ", "æ¼«ç”»", "ã‚¢ãƒ‹ãƒ¡", "ãƒ‡ã‚¶ã‚¤ãƒ³", "éŸ³æ¥½", "DTM", "ä½œæ›²", "å°èª¬", "å‰µä½œ"],
        exclude_keywords=["ç‚ä¸Š", "æ‰¹åˆ¤", "è¨´è¨Ÿ", "ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«", "æ”¿æ²»", "äº‹ä»¶"],
        anonymize=True,
    ),
}


def fetch_rss(url: str) -> list[dict]:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®RSSå–å¾—ã¯feedparserãŒå¿…è¦ï¼‰"""
    # æ³¨æ„: æœ¬ç•ªç’°å¢ƒã§ã¯feedparserã‚’ä½¿ç”¨
    # pip install feedparser
    try:
        import feedparser
        feed = feedparser.parse(url)
        items = []
        for entry in feed.entries[:10]:
            items.append({
                "title": entry.get("title", ""),
                "summary": entry.get("summary", ""),
                "link": entry.get("link", ""),
            })
        return items
    except ImportError:
        # feedparserãŒãªã„å ´åˆã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        return get_sample_news()


def get_sample_news() -> list[dict]:
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆé–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    return [
        {"title": "æ–°ã—ã„Webãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒãƒªãƒªãƒ¼ã‚¹", "summary": "é–‹ç™ºè€…å‘ã‘ã®æ–°ã—ã„ãƒ„ãƒ¼ãƒ«ãŒç™»å ´", "link": ""},
        {"title": "AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®æœ€æ–°å‹•å‘", "summary": "æ©Ÿæ¢°å­¦ç¿’ã‚’æ´»ç”¨ã—ãŸæ–°ã‚µãƒ¼ãƒ“ã‚¹", "link": ""},
        {"title": "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒæ³¨ç›®ã‚’é›†ã‚ã‚‹", "summary": "ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ä¸»å°ã®é–‹ç™ºãŒæ´»ç™ºã«", "link": ""},
        {"title": "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã®æ–°æ©Ÿèƒ½ç™ºè¡¨", "summary": "é–‹ç™ºè€…å‘ã‘ã®æ©Ÿèƒ½ãŒå……å®Ÿ", "link": ""},
        {"title": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã®æœ€æ–°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ", "summary": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã¨æ–°æ©Ÿèƒ½è¿½åŠ ", "link": ""},
    ]


def collect_news_for_reporter(reporter_id: str, config: ReporterConfig) -> list[NewsItem]:
    """è¨˜è€…ã”ã¨ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    all_items = []

    for source in config.sources:
        print(f"  Fetching from {source['name']}...")
        raw_items = fetch_rss(source["url"])

        for item in raw_items:
            title = item.get("title", "")
            summary = item.get("summary", "")

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if not config.should_include(title + " " + summary):
                continue

            news_item = NewsItem(
                id=f"news_{uuid.uuid4().hex[:8]}",
                title=title,
                summary=summary[:100] if summary else None,
                category=config.specialty.lower().replace("ãƒ»", "_"),
                source=reporter_id,
                original_url=item.get("link"),
                posted_at=datetime.now(),
                expires_at=datetime.now() + timedelta(days=2),
            )
            all_items.append(news_item)

    return all_items


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ“° Collecting news from all reporters...")

    bulletin_repo = BulletinRepository(Path("bots/data/bulletin_board"))

    # æœŸé™åˆ‡ã‚Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‰Šé™¤
    removed = bulletin_repo.cleanup_expired()
    if removed > 0:
        print(f"  Removed {removed} expired news items")

    total_added = 0

    for reporter_id, config in REPORTERS.items():
        print(f"\nğŸ“ {reporter_id} ({config.specialty}):")

        news_items = collect_news_for_reporter(reporter_id, config)

        for item in news_items[:5]:  # å„è¨˜è€…æœ€å¤§5ä»¶
            bulletin_repo.add_news_item(item)
            print(f"    + {item.title[:50]}...")
            total_added += 1

    print(f"\nâœ… Added {total_added} news items")

    # æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¡¨ç¤º
    recent = bulletin_repo.get_recent_news(5)
    print(f"\nğŸ“‹ Recent news ({len(recent)} items):")
    for item in recent:
        print(f"  - [{item.category}] {item.title[:40]}...")


if __name__ == "__main__":
    main()

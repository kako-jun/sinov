"""
ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
"""

import asyncio

from dotenv import load_dotenv

from .application import BotService
from .config import Settings
from .infrastructure import (
    NostrPublisher,
    OllamaProvider,
    ProfileRepository,
    StateRepository,
)


async def main() -> None:
    # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ï¼ˆ.env + .env.keysï¼‰
    load_dotenv()  # .env
    load_dotenv(".env.keys")  # .env.keysï¼ˆãƒœãƒƒãƒˆã®éµï¼‰

    # è¨­å®šèª­ã¿è¾¼ã¿
    settings = Settings()

    # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–
    llm_provider = None
    try:
        llm = OllamaProvider(settings.ollama_host, settings.ollama_model)
        if llm.is_available():
            llm_provider = llm
            print(f"âœ… Ollama is available (model: {settings.ollama_model})")
        else:
            print("âš ï¸  Ollama is not available")
    except Exception as e:
        print(f"âš ï¸  Could not connect to Ollama: {e}")

    if not llm_provider:
        print("âŒ LLM provider is required. Exiting.")
        return

    if settings.dry_run:
        print("\nğŸ” DRY RUN MODE: Posts will not be sent to the API\n")

    # ä¾å­˜é–¢ä¿‚ã‚’æ§‹ç¯‰
    publisher = NostrPublisher(settings.api_endpoint, settings.dry_run)
    profile_repo = ProfileRepository(settings.profiles_dir)
    state_repo = StateRepository(settings.states_file)

    # ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
    service = BotService(
        settings=settings,
        llm_provider=llm_provider,
        publisher=publisher,
        profile_repo=profile_repo,
        state_repo=state_repo,
    )

    # ãƒœãƒƒãƒˆèª­ã¿è¾¼ã¿
    await service.load_bots()

    # Nostrç½²åéµåˆæœŸåŒ–
    await service.initialize_keys()

    # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—é–‹å§‹
    await service.run_forever()


if __name__ == "__main__":
    asyncio.run(main())

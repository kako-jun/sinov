"""
ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
"""
import asyncio
import os
from pathlib import Path

from dotenv import load_dotenv

from .bot_manager import BotManager
from .llm import LLMClient


async def main() -> None:
    # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ï¼ˆ.env + .env.keysï¼‰
    load_dotenv()  # .env
    load_dotenv(".env.keys")  # .env.keysï¼ˆãƒœãƒƒãƒˆã®éµï¼‰
    
    # è¨­å®š
    profiles_dir = Path("bots/profiles")
    states_file = Path("bots/states.json")
    
    api_endpoint = os.getenv("API_ENDPOINT", "http://localhost:8787")
    dry_run = os.getenv("DRY_RUN", "false").lower() == "true"
    
    ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    ollama_model = os.getenv("OLLAMA_MODEL", "llama3.2:3b")
    
    # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    llm_client = None
    try:
        llm = LLMClient(ollama_host, ollama_model)
        if llm.is_available():
            llm_client = llm
            print(f"âœ… Ollama is available (model: {ollama_model})")
        else:
            print("âš ï¸  Ollama is not available, using simple content generation")
    except Exception as e:
        print(f"âš ï¸  Could not connect to Ollama: {e}")
        print("Using simple content generation instead")
    
    if dry_run:
        print("\nğŸ” DRY RUN MODE: Posts will not be sent to the API\n")
    
    # ãƒœãƒƒãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    manager = BotManager(
        profiles_dir=profiles_dir,
        states_file=states_file,
        api_endpoint=api_endpoint,
        relays=[],  # APIãŒå†…éƒ¨ã§ãƒªãƒ¬ãƒ¼ã«æ¥ç¶šã™ã‚‹ãŸã‚ä¸è¦
        llm_client=llm_client,
        dry_run=dry_run,
    )
    
    # ãƒœãƒƒãƒˆèª­ã¿è¾¼ã¿
    await manager.load_bots()
    
    # Nostrç½²åéµåˆæœŸåŒ–
    await manager.initialize_keys()
    
    # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—é–‹å§‹ï¼ˆ1åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯ï¼‰
    await manager.run_forever(check_interval=60)


if __name__ == "__main__":
    asyncio.run(main())

"""
ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆOllamaï¼‰
"""
from typing import Optional

import ollama


class LLMClient:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllamaï¼‰ã‚’ä½¿ã£ãŸæ–‡ç« ç”Ÿæˆ"""
    
    def __init__(self, host: str, model: str):
        self.host = host
        self.model = model
        self.client = ollama.Client(host=host)
    
    async def generate(self, prompt: str, max_length: Optional[int] = None) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰æ–‡ç« ã‚’ç”Ÿæˆ"""
        try:
            response = self.client.generate(
                model=self.model,
                prompt=prompt,
            )
            
            content = response["response"].strip()
            
            # æœ€å¤§é•·ã§ãƒˆãƒªãƒŸãƒ³ã‚°
            if max_length and len(content) > max_length:
                content = content[:max_length].rsplit(" ", 1)[0] + "..."
            
            return content
        except Exception as e:
            print(f"âš ï¸  LLM generation failed: {e}")
            return "ä»Šæ—¥ã‚‚é ‘å¼µã‚‹ ğŸ’ª"
    
    def is_available(self) -> bool:
        """OllamaãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            self.client.list()
            return True
        except Exception:
            return False
